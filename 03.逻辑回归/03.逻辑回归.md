## 逻辑回归 ##

### 一、概率 ###

定义：概率(P)robability，对一件事情发生的可能性的衡量，范围 0 <= P <= 1。

计算方法： 

- 根据个人置信
- 根据历史数据
- 根据模拟数据

条件概率：

![](https://i.imgur.com/yaFbBtW.png)

### 二、Logistic Regression (逻辑回归) ###

#### 1、例子 ####

当h(x) > 0.5时：

![](https://i.imgur.com/fF66Gb6.png)

当h(x) > 0.2时：

![](https://i.imgur.com/Dn7SBXD.png)

无法分类。

#### 2、基本模型 ####

测试数据为X（x0，x1，x2···xn），要学习的参数为： Θ（θ0，θ1，θ2，···θn）：

![](https://i.imgur.com/41eOMjM.jpg)

向量表示：

![](https://i.imgur.com/xjkY21D.jpg)

处理二值数据，引入Sigmoid函数时曲线平滑化：

![](https://i.imgur.com/LJYz9t5.jpg)

![](https://i.imgur.com/Ez5J6mm.jpg)

预测函数：

![](https://i.imgur.com/gQZ3YUy.jpg)

用概率表示：

正例(y=1)：

![](https://i.imgur.com/6sAbuMZ.jpg)

反例(y=0):

![](https://i.imgur.com/FXwCjDj.jpg)

Cost函数：

线性回归:

![](https://i.imgur.com/tt1TFjq.jpg)

![](https://i.imgur.com/V7m0wAL.jpg)

![](https://i.imgur.com/uNy1ftZ.jpg)

找到合适的 θ0，θ1使上式最小。

Logistic regression Cost函数:

![](https://i.imgur.com/DbFVZ9e.jpg)

目标：找到合适的 θ0，θ1使上式最小。

解法：梯度下降（gradient decent)：

![](https://i.imgur.com/1MDFEQx.jpg)
![](https://i.imgur.com/K3LnH6e.jpg)

![](https://i.imgur.com/EcAjozE.jpg)

更新法则：

![](https://i.imgur.com/e5VPEqc.jpg)

学习率：同时对所有的θ进行更新，重复更新直到收敛。

案例代码如下：

	import numpy as np
	import random
	
	
	def genData(numPoints,bias,variance):
	    x = np.zeros(shape=(numPoints, 2))
	    y = np.zeros(shape=(numPoints))
	    for i in range(0, numPoints):
	        x[i][0] = 1
	        x[i][1] = i
	        y[i] = (i+bias)+random.uniform(0, 1) + variance
	    return x, y
	
	
	def gradientDescent(x,y,theta,alpha,m,numIterations):
	    xTran = np.transpose(x)
	    for i in range(numIterations):
	        hypothesis = np.dot(x,theta)
	        loss = hypothesis-y
	        cost = np.sum(loss**2)/(2*m)
	        gradient=np.dot(xTran,loss)/m
	        theta = theta-alpha*gradient
	        print ("Iteration %d | cost :%f" %(i,cost))
	    return theta
	
	x,y = genData(100, 25, 10)
	print "x:"
	print x
	print "y:"
	print y
	
	m,n = np.shape(x)
	n_y = np.shape(y)
	
	print("m:"+str(m)+" n:"+str(n)+" n_y:"+str(n_y))
	
	numIterations = 100000
	alpha = 0.0005
	theta = np.ones(n)
	theta= gradientDescent(x, y, theta, alpha, m, numIterations)
	print(theta)