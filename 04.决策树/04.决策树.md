## 决策树 ##

### 一、决策树/判定树（decision tree)定义 ###

判定树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点：


![](https://i.imgur.com/EuXiqkE.png)

构造决策树的基本算法：

![](https://i.imgur.com/e5skNnq.png)

![](https://i.imgur.com/znDq5kZ.png)

### 二、熵（entropy） ###

信息和抽象，如何度量？

1948年，香农提出了 ”信息熵(entropy)“的概念。一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==>信息量的度量就等于不确定性的多少。
          
例如，猜世界杯冠军，假如一无所知，猜多少次？每个队夺冠的几率不是相等的。
          
所以用信息熵类度量信息的多少：

![](https://i.imgur.com/rAYyF1Q.png)

![](https://i.imgur.com/3N8OGrn.png)

变量的不确定性越大，熵也就越大。

### 三、决策树归纳算法 （ID3） ###

1970-1980， J.Ross. Quinlan, ID3算法选择属性判断结点信息获取量(Information Gain)：Gain(A) = Info(D) - Infor_A(D)，通过A来作为节点分类获取了多少信息。

例如：

![](https://i.imgur.com/znDq5kZ.png)

![](https://i.imgur.com/ZYnLTLw.png)

![](https://i.imgur.com/Eb1zHS9.png)

类似，Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048。所以，选择age作为第一个根节点：

![](https://i.imgur.com/0AwwSm8.png)

然后一直对每个分支使用同样的方法来构建树模型，算法如下：

- 树以代表训练样本的单个结点开始（步骤1）。
- 如果样本都在同一个类，则该结点成为树叶，并用该类标号（步骤2 和3）。
- 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性（步骤6）。该属- 性成为该结点的“测试”或“判定”属性（步骤7）。在算法的该版本中，
- 所有的属性都是分类的，即离散值。连续属性必须离散化。
- 对测试属性的每个已知的值，创建一个分枝，并据此划分样本（步骤8-10）。
- 算法使用同样的过程，递归地形成每个划分上的样本判定树。一旦一个属性出现在一个结点上，就不必该结点的任- 何后代上考虑它（步骤13）。
- 递归划分步骤仅当下列条件之一成立停止：
- (a) 给定结点的所有样本属于同一类（步骤2 和3）。
- (b) 没有剩余属性可以用来进一步划分样本（步骤4）。在此情况下，使用多数表决（步骤5）。
这涉及将给定的结点转换成树叶，并用样本中的多数所在的类标记它。替换地，可以存放结
点样本的类分布。
- (c) 分枝
- test_attribute = a i 没有样本（步骤11）。在这种情况下，以 samples 中的多数类
- 创建一个树叶（步骤12）

### 四、其他算法 ###

C4.5:  Quinlan
Classification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)
共同点：都是贪心算法，自上而下(Top-down approach)
区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain)。


### 五、树剪枝叶 （避免overfitting) ###

- 先剪枝
- 后剪枝

### 六、决策树的优缺点 ###

优点：直观，便于理解，小规模数据集有效。

缺点：处理连续变量不好、类别较多时，错误增加的比较快、可规模性一般。

### 七、案例 ###

给出某人的年龄、收入、是否为学生、信用情况来预测分析该人是否会买电脑，数据内容如下：

	RID,age,income,student,credit_rating,class_buys_computer
	1,youth,high,no,fair,no
	2,youth,high,no,excellent,no
	3,middle_aged,high,no,fair,yes
	4,senior,medium,no,fair,yes
	5,senior,low,yes,fair,yes
	6,senior,low,yes,excellent,no
	7,middle_aged,low,yes,excellent,yes
	8,youth,medium,no,fair,no
	9,youth,low,yes,fair,yes
	10,senior,medium,yes,fair,yes
	11,youth,medium,yes,excellent,yes
	12,middle_aged,medium,no,excellent,yes
	13,middle_aged,high,yes,fair,yes
	14,senior,medium,no,excellent,no

首先需要导入一些工具包：

	from sklearn.feature_extraction import DictVectorizer
	import csv
	from sklearn import tree
	from sklearn import preprocessing
	from sklearn.externals.six import StringIO

然后读取资源文件:

	allElectronicsData = open(r'F:\MachineLearning\MachinlearningPython/AllElectronics.csv', 'rb')

可以查看这个文件内容：

![](https://i.imgur.com/Zj9KFv7.png)

是一些训练集，关于预测哪些用户会买电脑。

然后读取文件头部：

	reader = csv.reader(allElectronicsData)
	headers = reader.next()

打印看看头部内容：

	print(headers)

![](https://i.imgur.com/77sk5RG.png)

这些正是训练集的样本参数项。reader将这些数据一行一行读入并保存。

然后做这些事：

	featureList = []//用于保存每行的特征值
	labelList = []//用于保存class_buys_computer项目的值
	
	for row in reader://逐行读入reader到row中
	    # print (row)
	    labelList.append(row[len(row)-1])//将row行的最后一个元素加入到labelList中，也就是保存class_buys_computer项目的值
	
	    rowDict = {}
	    for i in range(1, len(row)-1)://从row的第一项到最后一项开始遍历
	        rowDict[headers[i]] = row[i]//将对应的特征量的特征值数据保存到字典rowDict中
	    featureList.append(rowDict)//在featureList后添加字典信息

接下来打印看看这两个数组里的内容：

	print(labelList)
	print(featureList)

打印内容如下：

![](https://i.imgur.com/DbDSxzI.png)

![](https://i.imgur.com/BKk9Pao.png)

![](https://i.imgur.com/KldC3UQ.png)

。。。

接下来提取特征信息：

	# Vetorize features
	vec = DictVectorizer()
	dummyX = vec.fit_transform(featureList) .toarray()
	
	打印看看内容：
	print("dummyX: " + str(dummyX))//特征名字对应的特征值，bool类型
	print(vec.get_feature_names())//特征名字
	
	print("labelList: " + str(labelList))

![](https://i.imgur.com/4pbdmbk.png)

dummyX每行的值对应于vec.get_feature_names的特征信息。

接下来序列化判决：

	# vectorize class labels
	lb = preprocessing.LabelBinarizer()
	dummyY = lb.fit_transform(labelList)
	print("dummyY: " + str(dummyY))

![](https://i.imgur.com/AodCTcp.png)

对应于训练集表格的最后一列。

接下来创建分类器：

	# Using decision tree for classification
	# clf = tree.DecisionTreeClassifier()
	clf = tree.DecisionTreeClassifier(criterion='entropy')
	clf = clf.fit(dummyX, dummyY)
	print("clf: " + str(clf))

![](https://i.imgur.com/DXsnnFj.png)

接下来将分类器得到的数据保存下来，方便后续创建可视化节点图：

	# Visualize model
	#open cmd and input :dot -Tpdf  allElectronicInformationGainOri.dot -o output.pdf
	with open("allElectronicInformationGainOri.dot", 'w') as f:
	    f = tree.export_graphviz(clf, feature_names=vec.get_feature_names(), out_file=f)

创建数据决策树结构描述如下：

	digraph Tree {
	node [shape=box] ;
	0 [label="age=middle_aged <= 0.5\nentropy = 0.9403\nsamples = 14\nvalue = [5, 9]"] ;
	1 [label="student=no <= 0.5\nentropy = 1.0\nsamples = 10\nvalue = [5, 5]"] ;
	0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
	2 [label="credit_rating=excellent <= 0.5\nentropy = 0.7219\nsamples = 5\nvalue = [1, 4]"] ;
	1 -> 2 ;
	3 [label="entropy = 0.0\nsamples = 3\nvalue = [0, 3]"] ;
	2 -> 3 ;
	4 [label="income=low <= 0.5\nentropy = 1.0\nsamples = 2\nvalue = [1, 1]"] ;
	2 -> 4 ;
	5 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1]"] ;
	4 -> 5 ;
	6 [label="entropy = 0.0\nsamples = 1\nvalue = [1, 0]"] ;
	4 -> 6 ;
	7 [label="age=senior <= 0.5\nentropy = 0.7219\nsamples = 5\nvalue = [4, 1]"] ;
	1 -> 7 ;
	8 [label="entropy = 0.0\nsamples = 3\nvalue = [3, 0]"] ;
	7 -> 8 ;
	9 [label="credit_rating=excellent <= 0.5\nentropy = 1.0\nsamples = 2\nvalue = [1, 1]"] ;
	7 -> 9 ;
	10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1]"] ;
	9 -> 10 ;
	11 [label="entropy = 0.0\nsamples = 1\nvalue = [1, 0]"] ;
	9 -> 11 ;
	12 [label="entropy = 0.0\nsamples = 4\nvalue = [0, 4]"] ;
	0 -> 12 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
	}

接下来输入：dot -Tpdf  allElectronicInformationGainOri.dot -o output.pdf
生成pdf文件，文件内容如下：

![](https://i.imgur.com/SxQ5Bzi.png)

最后做个测试：

	oneRowX = dummyX[0, :]
	print("oneRowX: " + str(oneRowX))
	
	newRowX = oneRowX
	newRowX[0] = 1
	newRowX[2] = 0
	print("newRowX: " + str(newRowX))
	
	predictedY = clf.predict(newRowX)
	print("predictedY: " + str(predictedY))

![](https://i.imgur.com/My1ZvpM.png)