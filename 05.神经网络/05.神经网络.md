## 神经网络 ##

### 一、背景 ###

以人脑中的神经网络为启发，历史上出现过很多不同版本，最著名的算法是1980年的 backpropagation。

### 二、多层向前神经网络(Multilayer Feed-Forward Neural Network) ###

Backpropagation被使用在多层向前神经网络上，多层向前神经网络由以下部分组成：输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)。

![](https://i.imgur.com/sx9qxnR.png)

- 每层由单元(units)组成
- 输入层(input layer)是由训练集的实例特征向量传入
- 经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入
- 隐藏层的个数可以是任意的，输入层有一层，输出层有一层
- 每个单元(unit)也可以被称作神经结点，根据生物学来源定义
- 以上成为2层的神经网络（输入层不算）
- 一层中加权的求和，然后根据非线性方程转化输出
- 作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程

### 三、设计神经网络结构 ###

- 使用神经网络训练数据之前，必须确定神经网络的层数，以及每层单元的个数
- 特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）
- 离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值

比如：特征值A可能取三个值（a0, a1, a2), 可以使用3个输入单元来代表A。如果A=a0, 那么代表a0的单元值就取1, 其他取0；如果A=a1, 那么代表a1de单元值就取1，其他取0，以此类推。

- 神经网络即可以用来做分类(classification）问题，也可以解决回归(regression)问题

对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类），如果多余2类，每一个类别用一个输出单元表示，所以输入层的单元数量通常等于类别的数量。

没有明确的规则来设计最好有多少个隐藏层，根据实验测试和误差，以及准确度来实验并改进。

### 四、交叉验证方法(Cross-Validation) ###

![](https://i.imgur.com/4HBov3u.jpg)

### 五、Backpropagation算法 ###

- 通过迭代性的来处理训练集中的实例
- 对比经过神经网络后输入层预测值(predicted value)与真实值(target value)之间
- 反方向（从输出层=>隐藏层=>输入层）来以最小化误差(error)来更新每个连接的权重(weight)
- 算法详细介绍

输入：D：数据集，l 学习率(learning rate)， 一个多层前向神经网络

输入：一个训练好的神经网络(a trained neural network)

- 初始化权重(weights)和偏向(bias): 随机初始化在-1到1之间，或者-0.5到0.5之间，每个单元有一个偏向
- 对于每一个训练实例X，执行以下步骤：

由输入层向前传送：

![](https://i.imgur.com/8qZGu5Y.png)

![](https://i.imgur.com/ePigJVt.png)

![](https://i.imgur.com/PKJ7Y6q.png)

![](https://i.imgur.com/G85vAMt.png)

根据误差(error)反向传送：

对于输出层：![](https://i.imgur.com/2AgxRgA.png)

对于隐藏层：![](https://i.imgur.com/XYl25yj.png)

权重更新：

![](https://i.imgur.com/RXQZPvm.png)

偏向更新：

![](https://i.imgur.com/1y2ftqC.png)

终止条件：

- 权重的更新低于某个阈值
- 预测的错误率低于某个阈值
- 达到预设一定的循环次数

### 六、Backpropagation 算法举例 ###

![](https://i.imgur.com/PkVaig4.png)


![](https://i.imgur.com/PKJ7Y6q.png)

![](https://i.imgur.com/G85vAMt.png)

根据误差(error)反向传送：

对于输出层：![](https://i.imgur.com/2AgxRgA.png)

对于隐藏层：![](https://i.imgur.com/XYl25yj.png)

权重更新：

![](https://i.imgur.com/RXQZPvm.png)

偏向更新：

![](https://i.imgur.com/1y2ftqC.png)

![](https://i.imgur.com/rLgUaFj.png)

计算过程：

![](https://i.imgur.com/1hTDDVr.png)

### 七、实现一个简单的神经网络 ###

	# -*- coding: UTF-8 -*-
	import numpy as np
	
	
	# 双曲正切函数
	def tanh(x):
	    return np.tanh(x)
	
	
	# 双曲正切函数求导
	def tanh_deriv(x):
	    return 1.0 - np.tanh(x)*np.tanh(x)
	
	
	# 逻辑回归函数
	def logistic(x):
	    return 1/(1 + np.exp(-x))
	
	
	# 逻辑回归函数求导
	def logistic_derivative(x):
	    return logistic(x)*(1-logistic(x))
	
	
	class NeuralNetwork:
	    def __init__(self, layers, activation='tanh'):
	        """
	        :param layers: A list containing the number of units in each layer.
	        Should be at least two values
	        :param activation: The activation function to be used. Can be
	        "logistic" or "tanh"
	        """
	        if activation == 'logistic':
	            self.activation = logistic
	            self.activation_deriv = logistic_derivative
	        elif activation == 'tanh':
	            self.activation = tanh
	            self.activation_deriv = tanh_deriv
	
	        self.weights = []
	        for i in range(1, len(layers) - 1):
	            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)
	            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)
	
	        def printWeight():
	            print(self.weights)
	
	# X为二维训练集，y为预测标记，训练梯度l，抽样训练更新
	    def fit(self, X, y, learning_rate=0.2, epochs=10000):
	        X = np.atleast_2d(X)# 转化二维数组
	        temp = np.ones([X.shape[0], X.shape[1]+1])# X行， X+1列
	        temp[:, 0:-1] = X  # adding the bias unit to the input layer
	        X = temp
	        y = np.array(y)
	
	# 每次循环随机抽取一行训练样本，一行数据保存于a中
	        for k in range(epochs):
	            i = np.random.randint(X.shape[0])
	            a = [X[i]]
	# 完成正向所有更新
	            for l in range(len(self.weights)):  #going forward network, for each layer
	                a.append(self.activation(np.dot(a[l], self.weights[l])))  #Computer the node value for each layer (O_i) using activation function
	            error = y[i] - a[-1]  #Computer the error at the top layer
	            deltas = [error * self.activation_deriv(a[-1])] #For output layer, Err calculation (delta is updated error)
	
	            #Staring backprobagation
	            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer
	                #Compute the updated error (i,e, deltas) for each node going from top layer to input layer
	
	                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))
	            deltas.reverse()
	            for i in range(len(self.weights)):
	                layer = np.atleast_2d(a[i])
	                delta = np.atleast_2d(deltas[i])
	                self.weights[i] += learning_rate * layer.T.dot(delta)
	
	    def predict(self, x):
	        x = np.array(x)
	        temp = np.ones(x.shape[0]+1)
	        temp[0:-1] = x
	        a = temp
	        for l in range(0, len(self.weights)):
	            a = self.activation(np.dot(a, self.weights[l]))
	        return a
