## 支持向量机二 ##

### 一、SVM算法特性 ###

![](https://i.imgur.com/CdtBkK9.png)

- 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting
- SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。
- 一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。

### 二、线性不可分的情况 （linearly inseparable case) ###

![](https://i.imgur.com/Tw7T5TJ.png)

数据集在空间中对应的向量不可被一个超平面区分开，两个步骤来解决：

- 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中
- 在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理

![](https://i.imgur.com/M6mHqq9.jpg)

![](https://i.imgur.com/b7BMQxM.png)

如何利用非线性映射把原始数据转化到高维中？

例子：

3维输入向量：![](https://i.imgur.com/ASJdh6o.png)

转化到6维空间 Z 中去：![](https://i.imgur.com/yX1oK9q.png)
![](https://i.imgur.com/LDs7jVI.png)

新的决策超平面：![](https://i.imgur.com/Qm5LWrl.png)，其中W和Z是向量，这个超平面是线性的解出W和b之后，并且带入回原方程：

![](https://i.imgur.com/RYWu1Rc.png)

思考问题：

- 如何选择合理的非线性转化把数据转到高纬度中？
- 如何解决计算内积时算法复杂度非常高的问题？

使用核方法（kernel trick)。

### 三、核方法（kernel trick) ###

动机：在线性SVM中转化为最优化问题时求解的公式计算都是以内积(dot product)的形式出现的![](https://i.imgur.com/BXv0Thq.png)，其中![](https://i.imgur.com/v5J7neU.png)是把训练集中的向量点转化到高维的非线性映射函数，因为内积的算法复杂度非常大，所以我们利用核函数来取代计算非线性映射函数的内积。

以下核函数和非线性映射函数的内积等同：

![](https://i.imgur.com/wYikIT7.png)

常用的核函数(kernel functions)：

- h度多项式核函数(polynomial kernel of degree h)：![](https://i.imgur.com/jk2MdJc.png)
- 高斯径向基核函数(Gaussian radial basis function kernel):![](https://i.imgur.com/nSnElum.png)
- S型核函数(Sigmoid function kernel): ![](https://i.imgur.com/wyOuQuf.png)

如何选择使用哪个kernel？根据先验知识，比如图像分类，通常使用RBF，文字不使用RBF；尝试不同的kernel，根据结果准确度而定。

核函数举例:

    假设定义两个向量： x = (x1, x2, x3); y = (y1, y2, y3)
    定义方程：f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3)

    K(x, y ) = (<x, y>)^2

    假设x = (1, 2, 3); y = (4, 5, 6). 
 
    f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
    f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
    <f(x), f(y)> = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024

	K(x, y) = (4  + 10 + 18 ) ^2 = 32^2 = 1024

同样的结果，使用kernel方法计算容易很多。