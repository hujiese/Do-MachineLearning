## KNN ##

Cover和Hart在1968年提出了最初的邻近算法，它是一种输入基于实例的学习(instance-based learning), 懒惰学习(lazy learning)，属于分类(classification)算法。

可以看下面这个例子：

![](https://i.imgur.com/zPwumB5.png)

通过上表信息中的打斗次数和接吻次数判断电影类型：

![](https://i.imgur.com/4BRBe1K.png)

### 一、算法详述 ###

步骤：

- 为了判断未知实例的类别，以所有已知类别的实例作为参照
- 选择参数K
- 计算未知实例与所有已知实例的距离
- 选择最近K个已知实例
- 根据少数服从多数的投票法则(majority-voting)，让未知实例归类为K个最邻近样本中最多数的类别

关于距离的衡量方法：

Euclidean Distance 定义：

![](https://i.imgur.com/wlDxGME.jpg)![](https://i.imgur.com/xzXrM8k.png)

其他距离衡量：余弦值（cos）, 相关度 （correlation）, 曼哈顿距离 （Manhattan distance）

算法优点：简单、易于理解、容易实现、通过对K的选择可具备丢噪音数据的健壮性。

算法缺点：需要大量空间储存所有已知实例、算法复杂度高（需要比较所有已知实例与要分类的实例）、当其样本分布不平衡时，比如其中一类样本过大（实例数量过多）占主导的时候，新的未知实例容易被归类为这个主导样本，因为这类样本实例的数量过大，但这个新的未知实例实际并木接近目标样本。

### 二、案例 ###

水仙花有：萼片长度，萼片宽度，花瓣长度，花瓣宽度(sepal length, sepal width, petal length and petal width）四个特征；有Iris setosa, Iris versicolor, Iris virginica三种类型，现在需要根据这四个特征分类水仙花品种：

![](https://i.imgur.com/SXSgrJc.jpg)

#### 1.使用sklearn中国的KNN模块 ####

	from sklearn import neighbors
	from sklearn import datasets

加载KNN分类器：

	knn = neighbors.KNeighborsClassifier()

读取虹膜数据集：

	iris = datasets.load_iris()
	# save data
	# f = open("iris.data.csv", 'wb')
	# f.write(str(iris))
	# f.close()

打印数据集：

	print iris

数据如下：

	{'target_names': array(['setosa', 'versicolor', 'virginica'], 
	      dtype='|S10'), 'data': array([[ 5.1,  3.5,  1.4,  0.2],
	       [ 4.9,  3. ,  1.4,  0.2],
	       [ 4.7,  3.2,  1.3,  0.2],
	       [ 4.6,  3.1,  1.5,  0.2],
	       [ 5. ,  3.6,  1.4,  0.2],
	       [ 5.4,  3.9,  1.7,  0.4],
				......

		   [ 6.5,  3. ,  5.2,  2. ],
		   [ 6.2,  3.4,  5.4,  2.3],
		   [ 5.9,  3. ,  5.1,  1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
		       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
		       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
		       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
		       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
		       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
		       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'DESCR': 'Iris Plants Database\n\nNotes\n-----\nData Set Characteristics:\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThis is a copy of UCI ML iris datasets.\nhttp://archive.ics.uci.edu/ml/datasets/Iris\n\nThe famous Iris database, first used by Sir R.A Fisher\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\nReferences\n----------\n   - Fisher,R.A. "The use of multiple measurements in taxonomic problems"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n     Mathematical Statistics" (John Wiley, NY, 1950).\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']}

建立数据模型对象，传入数据训练样本和分类目标:

	knn.fit(iris.data, iris.target)

预测:

	predictedLabel = knn.predict([[0.1, 0.2, 0.3, 0.4]])

打印预测结果:

	print predictedLabel

打印结果如下：

	[0]


#### 2.手动编写实现 ####

	import csv
	import random
	import math
	import operator
	
	
	def loadDataset(filename, split, trainingSet = [], testSet = []):
	    with open(filename, 'rb') as csvfile:
	        lines = csv.reader(csvfile)
	        dataset = list(lines)
	        for x in range(len(dataset)-1):
	            for y in range(4):
	                dataset[x][y] = float(dataset[x][y])
	            if random.random() < split:
	                trainingSet.append(dataset[x])
	            else:
	                testSet.append(dataset[x])
	
	
	def euclideanDistance(instance1, instance2, length):
	    distance = 0
	    for x in range(length):
	        distance += pow((instance1[x]-instance2[x]), 2)
	    return math.sqrt(distance)
	
	
	def getNeighbors(trainingSet, testInstance, k):
	    distances = []
	    length = len(testInstance)-1
	    for x in range(len(trainingSet)):
	        #testinstance
	        dist = euclideanDistance(testInstance, trainingSet[x], length)
	        distances.append((trainingSet[x], dist))
	        #distances.append(dist)
	    distances.sort(key=operator.itemgetter(1))
	    neighbors = []
	    for x in range(k):
	        neighbors.append(distances[x][0])
	        return neighbors
	
	
	def getResponse(neighbors):
	    classVotes = {}
	    for x in range(len(neighbors)):
	        response = neighbors[x][-1]
	        if response in classVotes:
	            classVotes[response] += 1
	        else:
	            classVotes[response] = 1
	    sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)
	    return sortedVotes[0][0]
	
	
	def getAccuracy(testSet, predictions):
	    correct = 0
	    for x in range(len(testSet)):
	        if testSet[x][-1] == predictions[x]:
	            correct += 1
	    return (correct/float(len(testSet)))*100.0
	
	
	def main():
	    #prepare data
	    trainingSet = []
	    testSet = []
	    split = 0.67
	    loadDataset(r'irisdata.txt', split, trainingSet, testSet)
	    print 'Train set: ' + repr(len(trainingSet))
	    print 'Test set: ' + repr(len(testSet))
	    #generate predictions
	    predictions = []
	    k = 3
	    for x in range(len(testSet)):
	        # trainingsettrainingSet[x]
	        neighbors = getNeighbors(trainingSet, testSet[x], k)
	        result = getResponse(neighbors)
	        predictions.append(result)
	        print ('>predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))
	    print ('predictions: ' + repr(predictions))
	    accuracy = getAccuracy(testSet, predictions)
	    print('Accuracy: ' + repr(accuracy) + '%')
	
	
	if __name__ == '__main__':
	    main()

结果如下：

	Train set: 112
	Test set: 38
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-setosa', actual='Iris-setosa'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-virginica', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-versicolor', actual='Iris-versicolor'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-versicolor', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-versicolor', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	>predicted='Iris-virginica', actual='Iris-virginica'
	predictions: ['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica']
	Accuracy: 92.10526315789474%

